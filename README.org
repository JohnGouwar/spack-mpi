#+TITLE: spack-mpi: Distributed builds over MPI
#+AUTHOR: John Gouwar
*/Disclaimer:/* This is a work in progress and not ready for production use
* Pre-requisites
- Python>=3.13 (this may work on older Python versions, but is developed on 3.13)
- A working Spack>=1.0 installation
- My [[https://www.github.com/JohnGouwar/spack-extension-packages][spack-extension-packages]] repo that provides a few of the packages necessary
  for working with this extension
- An implementaion of MPI>=1.0 and a compatible build of the ~mpi4py~ package.
- GCC>=11 

The following Spack environment will provide all necessary prequisites:
#+begin_src yaml :tangle env/spack.yaml
  spack:
    config:
      extensions:
        - ../
    specs:
    - 'python@3.12:'
    - clingo@spack
    - "clustcc-gcc ^gcc@11:"
    - py-mpi4py
    - py-epic-ipc
    - mpich # or any other MPI implementation
    - flux-sched # site may already provide flux
    view: true
    concretizer:
      unify: true
    repos:
      extensions:
        git: "https://github.com/JohnGouwar/spack-extension-packages.git"
    toolchains:
      clustcc_toochain:
        - spec: "%c=clustcc-gcc"
        - when: "%c"
        - spec: "%cxx=clustcc-gcc"
        - when: "%cxx"
        - spec: "%fortran=gcc"
        - when: "%fortran"
#+end_src
Activating this environment in ~env/~ is necessary for running ~clustcc~
* clustcc Usage

** Configuration

~clustcc~ uses flux as its backing scheduler to enable heterogenous MPI
deployment, and thus requires a small amount of script scaffolding to work
effectively. ~clustcc~ options are configured through JSON: 
*** Configuration format
#+begin_src js
  {
      "slurm_partiton" : "partition", // name of slurm partition, "" if not using slurm
      "slurm_pmi" : "pmi2", // slurm pmi implementation, "" if not using slurm
      "slurm_time": "1:00:00", // time for slurm allocation, "" if not using slurm
      "slurm_batch_file": "clustcc.sbatch", // generated slurm batch file, "" if not using slurm
      "launch_file": "launch.sh", // generated flux launch file
      "total_cpus": 2, // total cpus across whole allocation
      "flux_exe": "/path/to/flux", // just flux if flux in PATH
      "spack_exe": "/path/to/spack/bin/spack", // just spack if spack in PATH
      "port_file": "/path/to/file/publishing/port", // head rank sets up
      // communication with this port, must be visible to all ranks (i.e. shared filesystem)
      "specs": "zlib%clustcc hdf5%clustcc" // specs to be installed with clustcc
      "num_workers": 1 // number of worker ranks
      "head_rank_cpus": 1, // number of cpus allocated to the head rank
      "head_rank_tasks": 1, // number of concurrent compilation tasks directly
      // executed by the head rank
      "worker_rank_cpus": 1,// number of cpus allocated to each worker rank
      "logging_level": "debug", // acceptable values:
      "logging_prefix": "/path/to/where/logs/should/be/stored"
  }
#+end_src
*** Generating configuration and run scripts
Running ~spack clustcc config new NAME~ will generate a skeleton of the above
JSON at ~NAME~. After populating the configuration file, running ~spack clustcc
config scripts NAME~ will generate the necessary scripts to actually execute
clustcc.

** Running clustcc on a cluster
If running over slurm, running ~sbatch generated.sbatch~ should be sufficient.
If running over flux, ~flux batch [OPTIONS] generated.sh~ should be sufficient.

** Running clustcc locally for debugging
The provided ~run.sh~ script runs MPI locally to aid in quick debugging.
~simple-c-package~ is a great smoketest, ~zlib%clustcc_toolchain ^gmake%clustcc_toolchain~,
can be a nice integration test.

* TODOs
- I would like to move to the new installer
- I have some linking ideas, but those are on the back burner.
- At some point I should introduce ccache, but that shouldn't be to hard.
# Local Variables:
# eval: (add-hook 'after-save-hook 'john/org-tangle-with-prompt nil t)
# End:
